


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import re
import seaborn as sns











coffe_index = pd.read_csv("data/merged_data_cleaned.csv", index_col = 0)


coffe_index.head(5)


coffe_index.shape


print("Observations:", coffe_index.shape[0])
print("Features:", coffe_index.shape[1])
print("Columns:", coffe_index.columns.tolist())





coffe_index.dtypes








coffe_index.columns


# I will use a lmbda function to convert the column names to lower case and snake_case
coffe_index = coffe_index.rename(columns = lambda col: col.lower().replace(".", "_"))


coffe_index.columns





coffe_index.bag_weight


coffe_index.bag_weight.dtype


coffe_index.bag_weight.value_counts()








# Normalize casting and spaces
coffe_index["bag_weight"] = coffe_index["bag_weight"].str.lower().str.replace(",", " ").str.strip()


coffe_index["bag_weight"]


# As a next step, I will separate the weigth and the units. I want to see what is the clear weight of each 
#observation. To do this I will extract the weight as a float and keep it in a new feature column called
#bag_weight_num.

coffe_index["bag_weight_num"] = coffe_index["bag_weight"].str.extract(r'(\d+\.?\d*)')[0].astype(float)


coffe_index["bag_weight_num"]


#Extract the unit

# As a next step I will extract the unit from the bag_weight column and store it in a new column bag_weight_unit
# After analysis pf the the mix entries containing kg and lbs, my desicion is to convert those to kg, because 
#the first entry in both observations is kg. The lbs entries are aftert the kg and probabbly are a techincal error.

coffe_index["bag_weight_unit"] = coffe_index["bag_weight"].str.extract(r'\d+\.?\d*\s*([a-z]*)')[0]


coffe_index["bag_weight_unit"].value_counts()


# Next steps:
# Analyze the observations without/missing entries
# Try to find out from which country they are and check what is the units used in that country
# Replace the missing units with the units used in that perticular country 

coffe_index[~coffe_index['bag_weight_unit'].isin(['kg', 'lbs'])]


coffe_index[coffe_index["country_of_origin"] == "United States (Hawaii)"].head(5)


coffe_index[coffe_index["country_of_origin"] == "Mexico"].head(5)


coffe_index[coffe_index["country_of_origin"] == "Guatemala"].head(5)


coffe_index[coffe_index["country_of_origin"] == "Nicaragua"]


coffe_index[coffe_index["country_of_origin"] == "Ethiopia"].head(5)





# List of indexes you want to update to lbs
indexes_to_update = [13, 51, 313, 606, 933]

# Set 'lbs' for those rows in the 'bag_weight_unit' column
coffe_index.loc[indexes_to_update, "bag_weight_unit"] = 'lbs'


coffe_index.loc[indexes_to_update, ['bag_weight_num', 'bag_weight_unit']]


# Convert the lbs to kg

# Ou analysis shows that 90 % of the units are in kg. Taking this into account and according to the data
# in internet for the measurement of coffe bags which is usually bags of 69 kg, we decide to convert all lb and lbs(they both means pounds)
# to kilograms.

coffe_index["bag_weight_kg"] = np.where(
    coffe_index["bag_weight_unit"].isin(['lb', 'lbs']),
    coffe_index["bag_weight_num"]* 0.453592,
    coffe_index["bag_weight_num"]
)


coffe_index["bag_weight_kg"].value_counts(dropna=False) 


coffe_index["bag_weight_num"]


coffe_index["bag_weight_unit"]


# Delete column bag_weight_kg     
coffe_index = coffe_index.drop(columns=["bag_weight_kg"])





coffe_index.harvest_year.value_counts(dropna=False)


coffe_index.harvest_year.dtypes


coffe_index.harvest_year.isna().sum()


coffe_index["harvest_year"].isna().mean() * 100








# Develop a function to clean the data 

def extract_clean_year(value):
    if pd.isna(value):
        return "Unknown"
    value = str(value).lower().strip()

   # Fix malformed: extract last 4 digits if there's a T followed by 4+ digits
    malformed_match = re.match(r'.*t.*(\d{4})$', value)
    if malformed_match:
        return malformed_match.group(1)

    # Standard YYYY
    match = re.match(r'^\d{4}$', value)
    if match:
        return value

    # YYYY/YYYY or YYYY-YYYY or YYYY / YYYY
    match = re.search(r'(\d{4})\s*[/\-]\s*(\d{4})', value)
    if match:
        return match.group(2)

    # YY/YY like 08/09 crop
    match = re.search(r'(\d{2})/(\d{2})', value)
    if match:
        return f"20{match.group(2)}"

    # Fallback: any 4-digit year
    match = re.search(r'(\d{4})', value)
    if match:
        return match.group(1)

    return "Unknown" 

coffe_index["harvest_year_clean"]= coffe_index["harvest_year"].apply(extract_clean_year)


coffe_index["harvest_year_clean"].value_counts(dropna=False)


coffe_index.iloc[948]


coffe_index.columns


# Drop the harvest_year


# Fixing the expiration dates, and grading dates to dates using pd.to_datetime


coffe_index.expiration.value_counts()


# Remove ordinal suffixes using regex
def clean_date(date_str):
    if pd.isna(date_str):
        return None
    # Strip whitespace and newlines
    date_str = date_str.strip()
    # Remove 'st', 'nd', 'rd', 'th' from day part
    return re.sub(r'(\d{1,2})(st|nd|rd|th)', r'\1', date_str)

# Apply the cleaning function
coffe_index["expiration_clean"] = coffe_index["expiration"].apply(clean_date)

# Convert the cleaned expiration column to datetime
coffe_index["expiration_clean"] = pd.to_datetime(coffe_index["expiration_clean"], errors="coerce")

# Apply the cleaning function
coffe_index["granding_date_clean"] = coffe_index["grading_date"].apply(clean_date)

# Convert the cleaned expiration column to datetime
coffe_index["granding_date_clean"] = pd.to_datetime(coffe_index["granding_date_clean"], errors="coerce")


coffe_index["expiration_clean"].isna().sum()


coffe_index.grading_date.value_counts(dropna=False)


coffe_index["granding_date_clean"].isna().sum()





coffe_index.country_of_origin.value_counts()


coffe_index.country_of_origin.value_counts(dropna=False)


coffe_index.country_of_origin.isna().value_counts()


unknown_countries = coffe_index[coffe_index["country_of_origin"].isna()]


unknown_countries





coffe_index[coffe_index["owner"] == "racafe & cia s.c.a"]





# We will replace the NaN with Columbia






coffe_index.owner.value_counts()


coffe_index.owner.info()


coffe_index.owner_1.value_counts()


# Compare the two columns 
# First we will make the text in the two columns lowercase and strip whitespace for comparision
coffe_index["owner_clean"]= coffe_index["owner"].str.lower().str.strip()
coffe_index["owner_1_clean"]= coffe_index["owner_1"].str.lower().str.strip()


coffe_index.owner_clean.value_counts() 


coffe_index.owner_1_clean.value_counts()


# Compare the clean columns
coffe_index["compare"] = coffe_index["owner_clean"] == coffe_index["owner_1_clean"]


# See in how many values differ
coffe_index["compare"].value_counts()


# List the rows which differ
differences = coffe_index[coffe_index["compare"] == False][["owner_clean", "owner_1_clean"]]


differences





owner_set = set(coffe_index['owner_clean'].dropna().unique())
owner1_set = set(coffe_index['owner_1_clean'].dropna().unique())

only_in_owner = owner_set - owner1_set
only_in_owner1 = owner1_set - owner_set


only_in_owner


only_in_owner1


coffe_index.producer.value_counts()





coffe_index["producer_clean"]= coffe_index["producer"].str.lower().str.strip()


# Compare the two and create a new column called relation
coffe_index["relation"] = coffe_index["producer_clean"] == coffe_index["owner_clean"]


coffe_index.relation.value_counts()


coffe_index[coffe_index["relation"]]





relation_counts = coffe_index["relation"].value_counts()


plt.bar(relation_counts.index.astype(str), relation_counts.values)
plt.show()


coffe_index["relation"].value_counts().plot(kind="bar", color = ["green", "orange"])
plt.xlabel("Are Producer and Owner the Same?")
plt.ylabel("Count")
plt.title("Producer vs Owner Relationship")
plt.xticks([0, 1], ["Different", "Same"])
plt.show()


sns.countplot(data=coffe_index, x="relation", hue="relation", palette="Set2", legend=False)
plt.title("Count of Producer-Owner Relationship")
plt.xlabel("Are Producer and Owner the Same?")
plt.ylabel("Count")
plt.xticks([0, 1], ["Different", "Same"])
plt.show()








coffe_index.color.value_counts()


# Create a pivot table with countries on the rows and including missing values

coffe_index_color = coffe_index.pivot_table(index = "country_of_origin", columns = "color", values = "species", 
                                            aggfunc = "count")


coffe_index_color.reset_index()


coffe_index_color.columns


coffe_index.columns


coffe_index.country_of_origin.value_counts()





# Make the dictionary
continent_map = {
    "Brazil": "South America",
    "Burundi": "Africa",
    "China": "Asia",
    "Colombia": "South America",
    "Costa Rica": "North America",
    "Cote d?Ivoire": "Africa",
    "Ecuador": "South America",
    "El Salvador": "North America",
    "Ethiopia": "Africa",
    "Guatemala": "North America",
    "Haiti": "North America",
    "Honduras": "North America",
    "India": "Asia",
    "Indonesia": "Asia",
    "Japan": "Asia",
    "Kenya": "Africa",
    "Laos": "Asia",
    "Malawi": "Africa",
    "Mauritius": "Africa",
    "Mexico": "North America",
    "Myanmar": "Asia",
    "Nicaragua": "North America",
    "Panama": "North America",
    "Papua New Guinea": "Oceania",
    "Peru": "South America",
    "Philippines": "Asia",
    "Rwanda": "Africa",
    "Taiwan": "Asia",
    "Tanzania, United Republic Of": "Africa",
    "Thailand": "Asia",
    "Uganda": "Africa",
    "United States": "North America",
    "United States (Hawaii)": "North America",
    "United States (Puerto Rico)": "North America",
    "Vietnam": "Asia"
}


# Make the new column - continents
coffe_index["continents"]= coffe_index["country_of_origin"].map(continent_map)


# Create a pivot table with continents in the rows and including missing values

coffe_continets_color = coffe_index.pivot_table(index = "continents", columns = "color", values = "species", 
                                            aggfunc = "count")


coffe_continets_color





coffe_index.aroma.value_counts()


coffe_index.flavor.value_counts()


coffe_index.moisture.value_counts()


# Select the relevant columns, which we what to explore

columns_to_explore = ["aroma", "flavor", "aftertaste", "acidity","body", "balance", "uniformity",
       "clean_cup", "sweetness", "cupper_points", "total_cup_points", "moisture"]

# Compute the mean and range for each

for col in columns_to_explore:
    mean_val = coffe_index[col].mean()
    min_val = coffe_index[col].min()
    max_val = coffe_index[col].max()
    value_range = max_val - min_val

# Print  
    print(f"--- {col.capitalize()} ---")
    print(f"Mean: {mean_val:.2f}")
    print(f"Min: {min_val}")
    print(f"Max: {max_val}")
    print(f"Range: {value_range:.2f}\n")

# Compute correlation matrix
correlation_matrix = coffe_index[columns_to_explore].corr()

# Display correlation matrix
print("\n=== Correlation Matrix ===")
print(correlation_matrix)

# Optional: visualize correlation matrix as a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", square=True, linewidths=0.5)
plt.title("Correlation Heatmap of Coffee Attributes")
plt.tight_layout()
plt.show()


for col in columns_to_explore:
    plt.figure(figsize=(10, 4))
    sns.histplot(coffe_index[col], kde=True, bins=30)
    plt.title(f"Distribution of {col.capitalize()}")
    plt.xlabel(col.capitalize())
    plt.ylabel("Frequency")
    plt.show()


sns.set(style="whitegrid")

for col in columns_to_explore:
    plt.figure(figsize=(8, 2.5))
    sns.boxplot(x=coffe_index[col], color="skyblue")
    plt.title(f'Boxplot of {col.capitalize()}', fontsize=14)
    plt.xlabel(col.capitalize())
    plt.tight_layout()
    plt.show()








coffe_index.columns


coffe_index.region.value_counts().unique


print(coffe_index['region'].value_counts().to_dict())


coffe_index.company


region_country_counts = coffe_index.groupby(['region', 'country_of_origin']).size().reset_index(name='count')


region_country_counts_new = coffe_index.groupby('region')['country_of_origin'].nunique().reset_index()



suspicious_regions = region_country_counts[region_country_counts['num_countries'] > 1]
print(suspicious_regions)


sns.boxplot(data=coffe_index, x='country_of_origin', y='altitude_mean_meters')
plt.xticks(rotation=90)
plt.title("Altitude by Country")
plt.show()


region_country_counts.iloc[80:100]


company_country = coffe_index.groupby(['company', 'country_of_origin']).size().reset_index(name='count')
company_country = company_country[company_country['count'] > 0]
suspicious = company_country.groupby('company')['country_of_origin'].nunique().reset_index()
suspicious = suspicious[suspicious['country_of_origin'] > 1]


coffe_index[coffe_index['company'].isin(suspicious['company'])].iloc[0:20]






pd.set_option('display.max_columns', None)
