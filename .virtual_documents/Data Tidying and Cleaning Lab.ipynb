


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import re
import seaborn as sns
import unicodedata











coffe_index = pd.read_csv("data/merged_data_cleaned.csv", index_col = 0)


coffe_index.head(5)


coffe_index.shape


print("Observations:", coffe_index.shape[0])
print("Features:", coffe_index.shape[1])
print("Columns:", coffe_index.columns.tolist())





coffe_index.dtypes








coffe_index.columns


# I will use a lmbda function to convert the column names to lower case and snake_case
coffe_index = coffe_index.rename(columns = lambda col: col.lower().replace(".", "_"))


coffe_index.columns





coffe_index.bag_weight


coffe_index.bag_weight.dtype


coffe_index.bag_weight.value_counts()








# Normalize casting and spaces
coffe_index["bag_weight"] = coffe_index["bag_weight"].str.lower().str.replace(",", " ").str.strip()


coffe_index["bag_weight"]


# As a next step, I will separate the weigth and the units. I want to see what is the clear weight of each 
#observation. To do this I will extract the weight as a float and keep it in a new feature column called
#bag_weight_num.

coffe_index["bag_weight_num"] = coffe_index["bag_weight"].str.extract(r'(\d+\.?\d*)')[0].astype(float)


coffe_index["bag_weight_num"]


#Extract the unit

# As a next step I will extract the unit from the bag_weight column and store it in a new column bag_weight_unit
# After analysis pf the the mix entries containing kg and lbs, my desicion is to convert those to kg, because 
#the first entry in both observations is kg. The lbs entries are aftert the kg and probabbly are a techincal error.

coffe_index["bag_weight_unit"] = coffe_index["bag_weight"].str.extract(r'\d+\.?\d*\s*([a-z]*)')[0]


coffe_index["bag_weight_unit"].value_counts()


# Next steps:
# Analyze the observations without/missing entries
# Try to find out from which country they are and check what is the units used in that country
# Replace the missing units with the units used in that perticular country 

coffe_index[~coffe_index['bag_weight_unit'].isin(['kg', 'lbs'])]


coffe_index[coffe_index["country_of_origin"] == "United States (Hawaii)"].head(5)


coffe_index[coffe_index["country_of_origin"] == "Mexico"].head(5)


coffe_index[coffe_index["country_of_origin"] == "Guatemala"].head(5)


coffe_index[coffe_index["country_of_origin"] == "Nicaragua"]


coffe_index[coffe_index["country_of_origin"] == "Ethiopia"].head(5)





# List of indexes you want to update to lbs
indexes_to_update = [13, 51, 313, 606, 933]

# Set 'lbs' for those rows in the 'bag_weight_unit' column
coffe_index.loc[indexes_to_update, "bag_weight_unit"] = 'lbs'


coffe_index.loc[indexes_to_update, ['bag_weight_num', 'bag_weight_unit']]


# Convert the lbs to kg

# Ou analysis shows that 90 % of the units are in kg. Taking this into account and according to the data
# in internet for the measurement of coffe bags which is usually bags of 69 kg, we decide to convert all lb and lbs(they both means pounds)
# to kilograms.

coffe_index["bag_weight_kg"] = np.where(
    coffe_index["bag_weight_unit"].isin(['lb', 'lbs']),
    coffe_index["bag_weight_num"]* 0.453592,
    coffe_index["bag_weight_num"]
)


coffe_index["bag_weight_kg"].value_counts(dropna=False) 


coffe_index["bag_weight_num"]


coffe_index["bag_weight_unit"]


# Delete column bag_weight_kg     
coffe_index = coffe_index.drop(columns=["bag_weight_kg"])





coffe_index.harvest_year.value_counts(dropna=False)


coffe_index.harvest_year.dtypes


coffe_index.harvest_year.isna().sum()


coffe_index["harvest_year"].isna().mean() * 100








# Develop a extract_clean_year function to clean the data 

def extract_clean_year(value):
    if pd.isna(value):
        return "Unknown"
    value = str(value).lower().strip()

   # Fix malformed: extract last 4 digits if there's a T followed by 4+ digits
    malformed_match = re.match(r'.*t.*(\d{4})$', value)
    if malformed_match:
        return malformed_match.group(1)

    # Standard YYYY
    match = re.match(r'^\d{4}$', value)
    if match:
        return value

    # YYYY/YYYY or YYYY-YYYY or YYYY / YYYY
    match = re.search(r'(\d{4})\s*[/\-]\s*(\d{4})', value)
    if match:
        return match.group(2)

    # YY/YY like 08/09 crop
    match = re.search(r'(\d{2})/(\d{2})', value)
    if match:
        return f"20{match.group(2)}"

    # Fallback: any 4-digit year
    match = re.search(r'(\d{4})', value)
    if match:
        return match.group(1)

    return "Unknown" 

coffe_index["harvest_year_clean"]= coffe_index["harvest_year"].apply(extract_clean_year)


coffe_index["harvest_year_clean"].value_counts(dropna=False)


coffe_index.columns


# Fixing the expiration dates and grading dates to dates using pd.to_datetime


coffe_index.expiration.value_counts()





# Remove ordinal suffixes using regex
def clean_date(date_str):
    if pd.isna(date_str):
        return None
    # Strip whitespace and newlines
    date_str = date_str.strip()
    # Remove 'st', 'nd', 'rd', 'th' from day part
    return re.sub(r'(\d{1,2})(st|nd|rd|th)', r'\1', date_str)

# Apply the cleaning function
coffe_index["expiration_clean"] = coffe_index["expiration"].apply(clean_date)

# Convert the cleaned expiration column to datetime
coffe_index["expiration_clean"] = pd.to_datetime(coffe_index["expiration_clean"], errors="coerce")

# Apply the cleaning function
coffe_index["grading_date_clean"] = coffe_index["grading_date"].apply(clean_date)

# Convert the cleaned expiration column to datetime
coffe_index["grading_date_clean"] = pd.to_datetime(coffe_index["grading_date_clean"], errors="coerce")


coffe_index["expiration_clean"].isna().sum()


coffe_index.expiration_clean.value_counts()


coffe_index.grading_date_clean.value_counts(dropna=False)


coffe_index["grading_date_clean"].isna().sum()


coffe_index = coffe_index.drop(columns=["grading_date"])


coffe_index = coffe_index.drop(columns=["expiration"])





coffe_index.country_of_origin.value_counts()


coffe_index.country_of_origin.value_counts(dropna=False)


coffe_index.country_of_origin.isna().value_counts()


unknown_countries = coffe_index[coffe_index["country_of_origin"].isna()]


unknown_countries





coffe_index[coffe_index["owner"] == "racafe & cia s.c.a"]





# We will replace the NaN with Columbia






coffe_index.owner.value_counts()


coffe_index.owner.info()


coffe_index.owner_1.value_counts()





col_owner, col_owner1 = "owner", "owner_1"


def normalize_name(x: str) -> str | None:
    if pd.isna(x): 
        return np.nan
    s = str(x)
    s = unicodedata.normalize("NFKC", s)  # tidy Unicode
    s = s.strip()
    s = re.sub(r'\s+', ' ', s)           # collapse internal spaces
    s = s.rstrip('.')                     # trailing dot like "Inc."
    s_low = s.lower()
    return s_low

coffe_index["owner_norm"]  = coffe_index[col_owner].map(normalize_name)
coffe_index["owner1_norm"] = coffe_index[col_owner1].map(normalize_name)


# Quick audit: where do they match, differ, or fill each other?

both_na        = coffe_index["owner_norm"].isna() & coffe_index["owner1_norm"].isna()
one_fills      = coffe_index["owner_norm"].isna() ^ coffe_index["owner1_norm"].isna()
equal_norm     = (coffe_index["owner_norm"] == coffe_index["owner1_norm"]) & ~both_na
conflict_norm  = ~(both_na | one_fills | equal_norm)

print("Audit summary:")
print({
    "both_na": int(both_na.sum()),
    "one_fills": int(one_fills.sum()),
    "equal_norm": int(equal_norm.sum()),
    "conflict_norm": int(conflict_norm.sum()),
})


conflicts_df = coffe_index.loc[conflict_norm, [col_owner, col_owner1, "owner_norm", "owner1_norm"]]
# Save for manual review for analysis
conflicts_df.to_csv("owner_conflicts.csv", index=False)
print(conflicts_df)





def choose_owner(row):
    a, b = row["owner_norm"], row["owner1_norm"]
    if pd.isna(a) and pd.isna(b):
        return np.nan
    if pd.isna(a): 
        return b
    if pd.isna(b): 
        return a
    # both present
    if a == b:
        return a
    # conflict → prefer owner (a). Adjust rule if you prefer b or longest string, etc.
    return a

coffe_index["owner_clean"] = coffe_index.apply(choose_owner, axis=1)

# Track where the value came from
def owner_source(row):
    a, b = row["owner_norm"], row["owner1_norm"]
    if pd.isna(a) and pd.isna(b): return "none"
    if pd.isna(a): return "owner_1"
    if pd.isna(b): return "owner"
    if a == b: return "both_equal"
    return "owner_preferred"  # conflict

coffe_index["owner_source"] = coffe_index.apply(owner_source, axis=1)


coffe_index.owner_clean.value_counts()





cols_to_drop = [col_owner, col_owner1, "owner_norm", "owner1_norm", "owner_clean_orig_case"]
coffe_index = coffe_index.drop(columns=[c for c in cols_to_drop if c in coffe_index.columns])





def normalize_owner_display(x):
    if pd.isna(x): return np.nan
    s = str(x).strip()
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"\s+", " ", s)
    return s

def strip_accents(s: str) -> str:
    s = unicodedata.normalize("NFKD", s)
    return "".join(ch for ch in s if not unicodedata.combining(ch))

def canonicalize_owner_for_key(x):
    if pd.isna(x): return np.nan
    s = str(x).strip()
    s = unicodedata.normalize("NFKC", s)
    s = strip_accents(s).lower()
    s = re.sub(r"\s+", " ", s)
    # collapse common company suffix punctuation/variants if you want (optional)
    # s = re.sub(r"\bS\.?A\.?\b", "sa", s, flags=re.I)
    key = re.sub(r"[^a-z0-9]+", "", s)
    return key or np.nan

# --- 2) Create the missing OWNER KEY (this fixes your KeyError) ---
coffe_index["owner_key"] = coffe_index["owner_clean"].map(canonicalize_owner_for_key)

# --- 3) Producer: make clean + key (safe to re-run) ---
if "producer_clean" not in coffe_index.columns:
    coffe_index["producer_clean"] = coffe_index["producer"].map(normalize_owner_display)

coffe_index["producer_key"] = coffe_index["producer_clean"].map(canonicalize_owner_for_key)

# --- 4) Relation: same entity? ---
coffe_index["relation"] = (
    coffe_index["owner_key"].notna()
    & coffe_index["producer_key"].notna()
    & (coffe_index["owner_key"] == coffe_index["producer_key"])
)

# --- 5) Quick summary ---
print("Counts:\n", coffe_index["relation"].value_counts(dropna=False))
print(f"Share Owner==Producer: {coffe_index['relation'].mean():.1%}")

# --- 6) (Optional) Peek mismatches for QA ---
mismatches = coffe_index.loc[~coffe_index["relation"], ["producer", "owner_clean"]].head(10)
print("\nSample mismatches:\n", mismatches)


coffe_index["relation"].value_counts().plot(
    kind="bar", 
    color=["tomato", "seagreen"], 
    rot=0
)
plt.xticks([0,1], ["Different", "Same"])
plt.xlabel("Are Producer and Owner the Same?")
plt.ylabel("Count")
plt.title("Producer vs Owner Relationship")
plt.show()





coffe_index.color.value_counts()


# Create a pivot table with countries on the rows and including missing values

coffe_index_color = coffe_index.pivot_table(index = "country_of_origin", columns = "color", values = "species", 
                                            aggfunc = "count")


coffe_index_color.reset_index()


coffe_index_color.columns


coffe_index.country_of_origin.value_counts()





country_to_continent = {
    # North America
    "Mexico": "North America",
    "United States": "North America",
    "United States (Hawaii)": "North America",
    "United States (Puerto Rico)": "North America",
    "Haiti": "North America",
    "Panama": "North America",
    "Guatemala": "North America",
    "Honduras": "North America",
    "Costa Rica": "North America",
    "El Salvador": "North America",
    "Nicaragua": "North America",

    # South America
    "Colombia": "South America",
    "Brazil": "South America",
    "Peru": "South America",
    "Ecuador": "South America",

    # Africa
    "Ethiopia": "Africa",
    "Tanzania, United Republic Of": "Africa",
    "Uganda": "Africa",
    "Kenya": "Africa",
    "Malawi": "Africa",
    "Burundi": "Africa",
    "Rwanda": "Africa",
    "Zambia": "Africa",
    "Mauritius": "Africa",
    "Cote d?Ivoire": "Africa",

    # Asia
    "Taiwan": "Asia",
    "Thailand": "Asia",
    "China": "Asia",
    "India": "Asia",
    "Myanmar": "Asia",
    "Vietnam": "Asia",
    "Philippines": "Asia",
    "Laos": "Asia",
    "Japan": "Asia",
    "Indonesia": "Asia",

    # Oceania
    "Papua New Guinea": "Oceania",}



# Make the new column - continents
coffe_index["continents"]= coffe_index["country_of_origin"].map(country_to_continent)


# Create a pivot table with continents in the rows and including missing values
coffe_continets_color = coffe_index.pivot_table(index = "continents", columns = "color", values = "species", 
                                            aggfunc = "count")


coffe_continets_color





coffe_index.aroma.value_counts()


coffe_index.flavor.value_counts()


coffe_index.moisture.value_counts()


# Select the relevant columns, which we what to explore

columns_to_explore = ["aroma", "flavor", "aftertaste", "acidity","body", "balance", "uniformity",
       "clean_cup", "sweetness", "cupper_points", "total_cup_points", "moisture"]

# Compute the mean and range for each

for col in columns_to_explore:
    mean_val = coffe_index[col].mean()
    min_val = coffe_index[col].min()
    max_val = coffe_index[col].max()
    value_range = max_val - min_val

# Print  
    print(f"--- {col.capitalize()} ---")
    print(f"Mean: {mean_val:.2f}")
    print(f"Min: {min_val}")
    print(f"Max: {max_val}")
    print(f"Range: {value_range:.2f}\n")

# Compute correlation matrix
correlation_matrix = coffe_index[columns_to_explore].corr()

# Display correlation matrix
print("\n=== Correlation Matrix ===")
print(correlation_matrix)

# Optional: visualize correlation matrix as a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", square=True, linewidths=0.5)
plt.title("Correlation Heatmap of Coffee Attributes")
plt.tight_layout()
plt.show()


for col in columns_to_explore:
    plt.figure(figsize=(10, 4))
    sns.histplot(coffe_index[col], kde=True, bins=30)
    plt.title(f"Distribution of {col.capitalize()}")
    plt.xlabel(col.capitalize())
    plt.ylabel("Frequency")
    plt.show()


sns.set(style="whitegrid")

for col in columns_to_explore:
    plt.figure(figsize=(8, 2.5))
    sns.boxplot(x=coffe_index[col], color="skyblue")
    plt.title(f'Boxplot of {col.capitalize()}', fontsize=14)
    plt.xlabel(col.capitalize())
    plt.tight_layout()
    plt.show()








coffe_index.columns





coffe_index.region.value_counts().unique


region_country_counts = (
    coffe_index
    .groupby("region")["country_of_origin"]
    .nunique()
    .sort_values(ascending=False)
)

print(region_country_counts)





# To check countries against regions names I will use the following normalize function

def normalize(s):
    """Lowercase, strip, remove accents & extra spaces/punct for robust matching."""
    if pd.isna(s): return ""
    s = str(s).strip().lower()
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))  # strip accents
    s = re.sub(r"[^a-z0-9\s\-]", " ", s)  # drop punctuation/symbols
    s = re.sub(r"\s+", " ", s).strip()
    return s

# 1) Build a normalized country list FROM YOUR DATA (no pycountry)
country_unique = coffe_index["country_of_origin"].dropna().unique().tolist()
country_norm_set = {normalize(c): c for c in country_unique}  # map norm->original

# Add common country aliases you expect (optional)
aliases = {
    "tanzania": "Tanzania, United Republic Of",
    "ivory coast": "Cote d’Ivoire",
    "cote d'ivoire": "Cote d’Ivoire",
    "congo": "Congo, The Democratic Republic Of The",  # adapt if needed
    "russia": "Russian Federation",
    "uk": "United Kingdom",
    "usa": "United States",
    "u s a": "United States",
}
for k, v in aliases.items():
    country_norm_set.setdefault(k, v)

# 2) Flag regions that are actually country names (exact after normalization)
suspect_region_is_country = coffe_index[
    coffe_index["region"].fillna("").apply(lambda x: normalize(x) in country_norm_set)
][["region", "country_of_origin"]].drop_duplicates().sort_values("region")

# 3) Also flag regions that CONTAIN a country name as a substring (e.g., "asia pacific - philippines")
def contains_country_name(region):
    r = normalize(region)
    return any((" " + cn + " " in f" {r} ") or r == cn for cn in country_norm_set.keys())

suspect_region_contains_country = coffe_index[
    coffe_index["region"].fillna("").apply(contains_country_name)
][["region", "country_of_origin"]].drop_duplicates().sort_values("region")

# 4) Regions mapped to multiple countries (good for spotting human errors)
region_country_nunique = (
    coffe_index.groupby("region")["country_of_origin"].nunique().sort_values(ascending=False)
)
multi_country_regions = region_country_nunique[region_country_nunique > 1]

# 5) Heuristic: within each region, flag minority countries vs the majority for that region
def flag_minority_rows(df, region_col="region", country_col="country_of_origin", min_share=0.2):
    grp = df.groupby(region_col)[country_col].value_counts(normalize=True)
    minority = grp[grp <= min_share].reset_index(name="share")
    return df.merge(minority, on=[region_col, country_col], how="inner")

minority_region_country_rows = flag_minority_rows(coffe_index, min_share=0.25)[
    ["region", "country_of_origin", "share"]
].drop_duplicates().sort_values(["region", "share"])

# 6) Flag obvious macro-regions/continents mistakenly used as “region”
macro_regions = {
    "africa","asia","europe","americas","north america","south america","central america",
    "latin america","middle east","south asia","southeast asia","east asia","asia pacific",
    "pacific","caribbean","mena","eastern africa","western africa","east africa","west africa"
}
suspect_macro = coffe_index[
    coffe_index["region"].fillna("").apply(lambda x: normalize(x) in macro_regions)
][["region", "country_of_origin"]].drop_duplicates().sort_values("region")

# --- Quick prints to inspect ---
print("\n=== Regions that equal a country name ===")
print(suspect_region_is_country)

print("\n=== Regions that contain a country name ===")
print(suspect_region_contains_country)

print("\n=== Regions mapped to multiple countries (count of distinct countries) ===")
print(multi_country_regions.head(30))

print("\n=== Minority country assignments within a region (likely typos) ===")
print(minority_region_country_rows.head(30))

print("\n=== Macro/continent names used as regions ===")
print(suspect_macro)








altitude_summary = (
    coffe_index.groupby("country_of_origin")["altitude_mean_meters"]
    .agg(["min", "max", "mean", "count"])
    .sort_values("mean", ascending=False)
)
print(altitude_summary.head(15))


bad_altitudes = coffe_index[
    (coffe_index["altitude_mean_meters"] < 100) |
    (coffe_index["altitude_mean_meters"] > 3500)
][["country_of_origin", "region", "altitude_low_meters", "altitude_high_meters", "altitude_mean_meters"]]

print(bad_altitudes.head(20))


check_altitude = coffe_index[
    coffe_index["altitude_low_meters"].notna() & coffe_index["altitude_high_meters"].notna()
].copy()

check_altitude["expected_mean"] = (check_altitude["altitude_low_meters"] + check_altitude["altitude_high_meters"]) / 2
check_altitude["diff"] = check_altitude["altitude_mean_meters"] - check_altitude["expected_mean"]

# Show rows where preprocessing looks wrong
discrepancies = check_altitude[check_altitude["diff"].abs() > 10]
print(discrepancies[["country_of_origin","region","altitude_low_meters","altitude_high_meters","altitude_mean_meters","expected_mean","diff"]].head(20))



sns.boxplot(data=coffe_index, x='country_of_origin', y='altitude_mean_meters')
plt.xticks(rotation=90)
plt.title("Altitude by Country")
plt.show()


region_country_counts.iloc[80:100]





company_country_counts = (
    coffe_index.groupby("company")["country_of_origin"]
    .nunique()
    .sort_values(ascending=False)
)
print(company_country_counts.head(20))



company_region_pairs = (
    coffe_index[["company", "country_of_origin"]]
    .drop_duplicates()
    .sort_values("company")
)
print(company_region_pairs.head(30))



# Count countries per company
company_country_counts = (
    coffe_index.groupby("company")["country_of_origin"]
    .nunique()
    .reset_index(name="n_countries")
    .sort_values("n_countries", ascending=False)
)

# Filter: small/local companies with too many countries
suspect_companies = company_country_counts.query("n_countries > 2")
print(suspect_companies)



suspect_pairs = coffe_index[
    coffe_index["company"].isin(suspect_companies["company"])
][["company", "country_of_origin"]].drop_duplicates()
print(suspect_pairs.sort_values("company"))



company_country = coffe_index.groupby(['company', 'country_of_origin']).size().reset_index(name='count')
company_country = company_country[company_country['count'] > 0]
suspicious = company_country.groupby('company')['country_of_origin'].nunique().reset_index()
suspicious = suspicious[suspicious['country_of_origin'] > 1]


coffe_index[coffe_index['company'].isin(suspicious['company'])].iloc[0:20]






pd.set_option('display.max_columns', None)
